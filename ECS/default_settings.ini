[TrainingData]
# Полный путь к обучающему файлу
dataset =
test_percent =
# Полный путь к тестовому файлу
test_file =

[Experiment]
experiment_title =
rubricator =
binary =
threads =
n_folds =


[Preprocessing]
id = id_publ
title = title
text = ref_txt 
keywords = kw_list 
subj = SUBJ
ipv = IPV 
rgnti = RGNTI 
correct = eor
#
remove_stopwords = true
normalization = no
kw_delim = \
language = auto
batch_size = 5000


[WordEmbedding]
# Use existing model by file path
use_model =
# 50 by default
vector_dim = 50
pooling = sum
window = 5

[Classification]
models =

[logistic_regression]
penalty = l2
C = 1.0
solver = liblinear
fit_intercept = True
random_state = 
max_iter = 100


[svm]
C = 1.0
kernel = rbf
degree = 3
gamma =
coef0 = 0
random_state = 
max_iter = -1


[perceptron]
hidden_layer_sizes = [100]
solver = adam
activation = relu
alpha = 0.0001
learning_rate = constant
learning_rate_init = 0.001
power_t = 0.5
random_state =
max_iter = 200


[knn]
n_neighbors = 5
weights = uniform
p = 2


[decision_tree]
criterion = gini
splitter = best
# max_depth = None, но значение None пока не воспринимается парсером
# Поэтому поле пустое
max_depth =
min_samples_split = 2
min_samples_leaf = 1
max_features = auto
random_state =


[random_forest]
n_estimators = 10
criterion = gini
max_depth =
min_samples_split = 2
min_samples_leaf = 1
max_features = auto
random_state =


[adaboost]
n_estimators = 50
learning_rate = 1
algorithm = SAMME.R
random_state =